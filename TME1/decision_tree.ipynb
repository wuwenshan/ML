{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='red'> TME 1 - Arbres de décision, sélection de modèles </font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><code style=\"background:yellow; color:black\"> L’essentiel sur les arbres de décision </code></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un arbre de décision est un modèle de classification hiérarchique : pour des exemples sous la représentation $x = (x_1, x_2, . . . , x_d) ∈ R^d$, à chaque nœud de l’arbre est associé un test sur une des dimensions $x_i$ de la forme $x_i ≤ s$ ou $x_i > s$, avec $s$ une valeur réelle. Ce test indique le nœud fils qui doit être sélectionné (par exemple pour un arbre binaire, le fils gauche quand le test est vrai, le fils droit sinon). À chaque feuille de l’arbre est associée une étiquette. Ainsi, la classification d’un exemple consiste enune succession de tests sur les valeurs des dimensions de l’exemple, selon un chemin dans l’arbre de la racine à une des feuilles. La feuille atteinte donne la classe prédite. L’apprentissage de l’arbre s’effectue de manière récursive gloutonne top-down : à chaque nœud, l’algorithme doit choisir un test optimal, c’est-à-dire à la fois sur quelle dimension faire le test et quel seuillage appliqué (ce qu’on appelle un split). La mesure d’optimalité est en général une mesure d’homogénéité sur la partition obtenue, usuellement l’entropie de Shanon ou l’index de Gini : l’entropie d’une partition est d’autant plus petite qu’une classe prédomine dans chaque sous-ensemble de la partition, elle est nulle lorsque la séparation est parfaite (un seul label présent dans chacune des partitions) et maximale lorsque l’ensemble est le plus désordonné possible (équiprobabilité des labels dans chaque partition). Pour calculer le split optimal, chaque dimension de l’espace de description est considérée itérativement ; pour une dimension $i$, les exemples sont triés par rapport à la valeur de l’attribut de cette dimension $x_i$, puis pour chaque split possible, le calcul de l’homogénéité des deux partitions obtenues (en termes de label) par ce split est effectué. Le split ayant la meilleure homogénéité est alors choisi. Bien que l’algorithme pourrait continuer récursivement jusqu’à n’obtenir que des feuilles contenant un ensemble pur d’exemples (d’une seule classe - si les exemples sont séparables, i.e. il n’y a pas deux exemples de label opposé avec une même description), on utilise souvent des critères d’arrêts (pourquoi ? - nous y reviendrons lors de ce TP). Les plus utilisés sont le nombre d’exemples minimumque doit contenir un nœud pour être divisé (critère local) et la profondeur maximale de l’arbre (critère global)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1 - Entropie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Soit un objet itérable $vect$ (une liste ou un vecteur **numpy** par exemple) qui contient une liste de label. Coder une fonction $entropie (vect)$ qui calcule l’entropie de ce vecteur : $H(Y) = \\sum_{y∈Y} p_y*log(p_y)$, $p_y$ correspond à la probabilité du label $y$ dans le vecteur $vect$. Penser à utiliser l’objet $Counter$ du module $collections$ qui permet de faire un histogramme des éléments d’une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections as clt\n",
    "import pickle\n",
    "import pydot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_log_p(freq):\n",
    "    \"\"\" fonction pour calculer \\sum p_i log(p_i) \"\"\"\n",
    "    return np.nan_to_num(np.sum(freq*np.log2(freq)))\n",
    "\n",
    "def entropy(y):\n",
    "    \"\"\" calcul de l'entropie d'un ensemble\"\"\"\n",
    "    ylen = float(y.size)\n",
    "    if ylen <= 1:\n",
    "        return 0\n",
    "    freq = np.array(list(clt.Counter(y).values()))/ylen\n",
    "    return -p_log_p(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. L’entropie conditionnelle permet de calculer l’homogénéité de la partition obtenue. Dans le cas général d’un split n-aire en n partitions $P = {P_1, . . . , P_n}$, l’entropie conditionnelle à $P$ s’écrit $H(Y|P) = \\sum_{i}p(P_i)*H(Y|P_i)$, avec $H(Y|Pi) = -\\sum_{y∈Y}p(y|P_i)*log(p(y|P_i))$ l’entropie des labels conditionnée à la partition considérée, et $p(P_i) = |P_i| / \\sum_{j}P_j$, la proportion d’éléments dans $P_i$. <br>Coder la fonction entropie_cond(list_vect) qui à partir d’une liste de listes de labels (la partition des labels), calcule l’entropie conditionnelle de la partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_cond(y_list):\n",
    "    h, total = 0.,0.\n",
    "    for y in y_list:\n",
    "        h += len(y)*entropy(y)\n",
    "        total += len(y)\n",
    "    return h/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Le code suivant permet de charger un extrait de la base imdb (à télécharger sur le site de l’ue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4587\n"
     ]
    }
   ],
   "source": [
    "# data : tableau (films ,features)\n",
    "# id2titles : dictionnaire  id -> titre ,\n",
    "# fields : id  feature  -> nom\n",
    "[data , id2titles , fields ] = pickle.load(open(\"imdb_extrait.pkl\",\"rb\"))\n",
    "# la  derniere  colonne  est le vote\n",
    "datax = data [: ,:32]\n",
    "datay = np.array ([1 if x[33] >6.5  else  -1 for x in data])\n",
    "print(len(id2titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque ligne du tableau data correspond à la description d’un film (le titre dans $id2titles$), chaque colonne à un attribut (dont la signification est donnée par $fields$). La plupart sont des genres (action, comédie, ...), la valeur 1 indique l’appartenance du film au genre, 0 sinon. Les dernières colonnes concernent l’année de production, la durée du film, le budget, le nombre de vote et la note moyenne attribuée au film. On binarise la note moyenne afin d’avoir deux classes, les films de note supérieure à 6.5, et les autres (vecteur $datay$). <br> Calculer pour chaque attribut binaire l’entropie et l’entropie conditionnelle du vote selon la partition induite par l’attribut (les exemples dont la valeur de l’attribut est 1 vs les autres). Calculer également la différence entre l’entropie et l’entropie conditionnelle pour chaque attribut. A quoi correspond une valeur de 0 ? une valeur de 1 ? Quel est le meilleur attribut pour la première partition ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sci-Fi': [0.564976837274551, 0.7759428920883399, -0.21096605481378894],\n",
       " 'Crime': [0.7256620117131098, 0.8562854793076193, -0.1306234675945095],\n",
       " 'Romance': [0.7540946843905093, 0.870501815646319, -0.1164071312558097],\n",
       " 'Animation': [0.2634038042219715, 0.6251563755620502, -0.36175257134007865],\n",
       " 'Music': [0.1926285852674641, 0.5897687660847964, -0.3971401808173324],\n",
       " 'Comedy': [0.9487805443246595, 0.9678447456133942, -0.01906420128873465],\n",
       " 'War': [0.2653268692430364, 0.6261179080725825, -0.3607910388295461],\n",
       " 'Horror': [0.5347013539632184, 0.7608051504326735, -0.2261037964694551],\n",
       " 'Film-Noir': [0.040466328209344704, 0.5136876375557368, -0.47322130934639206],\n",
       " 'Adult': [-0.0, 0.49345447345106436, -0.49345447345106436],\n",
       " 'News': [-0.0, 0.49345447345106436, -0.49345447345106436],\n",
       " 'Reality-TV': [-0.0, 0.49345447345106436, -0.49345447345106436],\n",
       " 'Thriller': [0.9104758524619008, 0.9486923996820149, -0.03821654722011414],\n",
       " 'Western': [0.13310901662996316, 0.560008981766046, -0.4268999651360828],\n",
       " 'Mystery': [0.49425468209811785, 0.7405818145001233, -0.24632713240200543],\n",
       " 'Short': [0.00786173698629584, 0.49738534194421236, -0.4895236049579165],\n",
       " 'Talk-Show': [-0.0, 0.49345447345106436, -0.49345447345106436],\n",
       " 'Drama': [0.9999176833887986, 0.9934133151454636, 0.006504368243335001],\n",
       " 'Action': [0.8103265236207233, 0.8986177352614261, -0.08829121164070275],\n",
       " 'Documentary': [0.058438060605145864,\n",
       "  0.5226735037536373,\n",
       "  -0.4642354431484914],\n",
       " 'Musical': [0.17702726334400468, 0.5819681051230667, -0.4049408417790621],\n",
       " 'History': [0.22865199105519707, 0.6077804689786629, -0.3791284779234658],\n",
       " 'Family': [0.45026771320199777, 0.7185883300520632, -0.2683206168500655],\n",
       " 'Adventure': [0.6915486835207777, 0.8392288152114532, -0.1476801316906755],\n",
       " 'Fantasy': [0.5144914462944487, 0.7507001965982887, -0.23620875030384003],\n",
       " 'Game-Show': [-0.0, 0.49345447345106436, -0.49345447345106436],\n",
       " 'Sport': [0.19699814392010093, 0.5919535454111148, -0.3949554014910138],\n",
       " 'Biography': [0.3061179094676092, 0.6465134281848689, -0.3403955187172597]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getAllEntropies(datax, datay, fields):\n",
    "    allEnt = dict()\n",
    "    for key, value in fields.items():\n",
    "        if key <= 27:\n",
    "            ent = entropy(datax[:,key])\n",
    "            entCond = entropy_cond([datax[:,key], datay])\n",
    "            allEnt[value] = [ent, entCond, ent-entCond]\n",
    "    return allEnt\n",
    "\n",
    "getAllEntropies(datax, datay, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Si l'entropie a une valeur de 0 alors l'échantillon est homogène (seulement des 0 ou 1), si elle a une valeur de 1 alors l'échantillon est divisé en parts égales (autant de 0 que de 1). <br> \n",
    "Le gain d'information est calculé de la façon suivante : $entropy(T) - entropy$_$cond(T,X)$, plus cette valeur est grande plus l'information apportée par l'attribut $X$ sera pertinente et donc choisi pour faire le split. <br>\n",
    "Le meilleur attribut est \"drama\" avec un gain de 0.0065.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><code style=\"background:yellow; color:black\"> Quelques expériences préliminaires </code></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tous les modèles d’apprentissage que nous étudierons seront calqués (hérités, mais la notion d’héritage en python est optionnelle) sur la classe Classifier, munie d’un constructeur (qui permet d’initialiser les paramètres, variables d’instance de l’objet), d’une méthode **fit(data,labels)** qui permet d’apprendre le modèle sur les données en paramètre, d’une méthode **predict(data)** qui permet d’obtenir un vecteur de prédiction pour les données passées en paramètre, et d’une méthode **score(data,labels)** qui permet de renvoyer le pourcentage de bonne classification des données par rapport aux labels passés en paramètre. Le code suivant permet de créer, d’apprendre un arbre de décision et de l’utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Classes\n",
    "###############################\n",
    "\n",
    "\n",
    "class Classifier(object):\n",
    "    \"\"\" Classe generique d'un classifieur\n",
    "        Dispose de 3 méthodes :\n",
    "            fit pour apprendre\n",
    "            predict pour predire\n",
    "            score pour evaluer la precision\n",
    "    \"\"\"\n",
    "    def fit(self,data,y):\n",
    "        raise NotImplementedError(\"fit non  implemente\")\n",
    "    def predict(self,data):\n",
    "        raise NotImplementedError(\"predict non implemente\")\n",
    "    def score(self,data,y):\n",
    "        return (self.predict(data)==y).mean()\n",
    "\n",
    "\n",
    "class Split(object):\n",
    "    \"\"\" Permet de coder un split pour une variable continue\n",
    "    \"\"\"\n",
    "    def __init__(self,idvar=None,threshold=None,gain=None):\n",
    "        \"\"\"\n",
    "        :param idvar: numero de la variable de split\n",
    "        :param threshold: seuil\n",
    "        :param gain: gain d'information du split\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.idvar=idvar\n",
    "        self.threshold=threshold\n",
    "        self.gain=gain\n",
    "\n",
    "    def predict(self,data):\n",
    "        \"\"\" Prediction pour une matrice d'exemples, -1 si <= threshold, +1 sinon\n",
    "        :param x: matrice d'exemples\n",
    "        :return: vecteur des labels\n",
    "        \"\"\"\n",
    "        if len(data.shape)==1:\n",
    "            data=data.reshape((1,data.shape[0]))\n",
    "        return [-1 if data[i,self.idvar]<=self.threshold else 1 for i in range(data.shape[0])]\n",
    "\n",
    "    @staticmethod\n",
    "    def best_gain(x,y):\n",
    "        \"\"\"  calcul le meilleur seuil pour la colonne x (1-dimension) et les labels y\n",
    "        :param x: vecteur 1d des donnees\n",
    "        ;param y: vecteur des labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ylen = float(y.size)\n",
    "        idx_sorted = np.argsort(x)\n",
    "        h=entropy(y)\n",
    "        xlast=x[idx_sorted[0]]\n",
    "        split_val=x[idx_sorted[0]]\n",
    "        hmin = h\n",
    "        for i in range(y.size):\n",
    "            if x[idx_sorted[i]]!=xlast:\n",
    "                htmp = entropy_cond([y[idx_sorted[:i]], y[idx_sorted[i:]]])\n",
    "                if htmp<hmin:\n",
    "                    hmin=htmp\n",
    "                    split_val=(xlast+x[idx_sorted[i]])/2.\n",
    "            xlast=x[idx_sorted[i]]\n",
    "        return (h-hmin/ylen),split_val\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_split(data,y):\n",
    "        if len(data.shape)==1:\n",
    "            data = data.reshape((1,data.shape[0]))\n",
    "        hlist = [[Split.best_gain(data[:,i],y),i] for i in range(data.shape[1])]\n",
    "        (h,threshold),idx= max(hlist)\n",
    "        return Split(idx,threshold,h)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"var %s, thresh %f (gain %f)\" %(self.idvar,self.threshold, self.gain)\n",
    "\n",
    "class Node(Classifier):\n",
    "    \"\"\" Noeud d'un arbre\n",
    "    \"\"\"\n",
    "    def __init__(self,split=None,parent=None,left=None,right=None,leaf=True,depth=-1,label=None,**kwargs):\n",
    "        \"\"\"\n",
    "        :param split:  split du noeud\n",
    "        :param parent: noeud parent, None si root\n",
    "        :param left: fils gauche\n",
    "        :param right: fils droit\n",
    "        :param leaf: boolean vrai si feuille\n",
    "        :param depth: profondeur\n",
    "        :param label: label preponderant\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.split, self.parent, self.left, self.right, self.leaf, self.label, self.depth = \\\n",
    "                                            split, parent, left, right, leaf, label, depth\n",
    "        self.info = dict(kwargs)\n",
    "\n",
    "    def predict(self,data):\n",
    "        if len(data.shape)==1:\n",
    "            data=data.reshape((1,data.shape[0]))\n",
    "        if self.leaf:\n",
    "            return [self.label]*data.shape[0]\n",
    "        return [self.left.predict(data[i,:])[0] if res<0 else self.right.predict(data[i,:])[0]\n",
    "                for i, res in enumerate(self.split.predict(data))]\n",
    "\n",
    "    def fit(self, data, y):\n",
    "        counts=clt.Counter(y)\n",
    "        self.split=Split.find_best_split(data, y)\n",
    "        self.label = counts.most_common()[0][0]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.leaf:\n",
    "            return \"Leaf : %s\" % (self.label,)\n",
    "        return \"Node : %s (%s)\" % (self.split,self.info)\n",
    "\n",
    "class DecisionTree(Classifier):\n",
    "    \"\"\" Arbre de decision\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,max_depth=None,min_samples_split=2):\n",
    "        \"\"\"\n",
    "        :param max_depth: profondeur max\n",
    "        :param min_samples_split:  nombre d'exemples minimal pour pouvoir spliter le noeud\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.max_depth, self.min_samples_split = max_depth, min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self,data,y):\n",
    "        \"\"\" apprentissage de l'arbre de maniere iterative\n",
    "        on apprend un noeud, puis on cree les deux enfants de ce noeud, que l'on ajoute a la pile des noeuds\n",
    "        a traiter par la suite (nodes_to_treat), ainsi que les index des exemples associes (dic_idx)\n",
    "        \"\"\"\n",
    "        self.root = Node(depth=0)\n",
    "        nodes_to_treat = [self.root]\n",
    "        dic_idx = dict({self.root : range(len(y))})\n",
    "        while len(nodes_to_treat)>0:\n",
    "            # recuperation du noeud courant\n",
    "            curnode = nodes_to_treat.pop()\n",
    "            #recuperation de la liste des indices des exemples associes, x[idx_train,:] contient l'ensemble des\n",
    "            #exemples a traiter\n",
    "            idx_train = dic_idx.pop(curnode)\n",
    "            # infos complementaires sur le nombre d'exemples en apprentissage par label\n",
    "            for lab,clab in clt.Counter(y[idx_train]).items():\n",
    "                curnode.info[lab]=clab\n",
    "            curnode.fit(data[idx_train,:],y[idx_train])\n",
    "\n",
    "            # recupere les predictions pour partager entre fils droit et gauche les exemples\n",
    "            pred = curnode.split.predict(data[idx_train,:])\n",
    "            l_idx = [ idx_train[i] for i in range(len(idx_train)) if pred[i]<0 ]\n",
    "            r_idx = list(set(idx_train).difference(l_idx))\n",
    "\n",
    "            #Condition d'arrets\n",
    "            if entropy(y[idx_train])==0 or curnode.depth >= self.max_depth or \\\n",
    "                    len(l_idx) < self.min_samples_split or len(r_idx) < self.min_samples_split:\n",
    "                curnode.leaf=True\n",
    "                continue\n",
    "            #Creation des deux enfants\n",
    "            curnode.left = Node(parent=curnode,depth=curnode.depth+1)\n",
    "            curnode.right = Node(parent=curnode,depth=curnode.depth+1)\n",
    "            curnode.leaf = False\n",
    "            #On enregistre les indices correspondant aux deux noeuds\n",
    "            dic_idx[curnode.left]=l_idx\n",
    "            dic_idx[curnode.right]=r_idx\n",
    "            #On ajoute les deux enfants a la liste des noeuds a traiter\n",
    "            nodes_to_treat = [curnode.left,curnode.right]+nodes_to_treat\n",
    "\n",
    "    def predict(self,data):\n",
    "        return self.root.predict(data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.print_tree()\n",
    "\n",
    "    def to_dot(self,dic_var=None):\n",
    "        s=\"digraph Tree {\"\n",
    "        cpt=0\n",
    "        nodes = [(self.root,cpt)]\n",
    "        while len(nodes)>0:\n",
    "            curnode,idx = nodes.pop()\n",
    "            labinfo = \",\".join([\"%s: %s\" % (lab,slab) for lab,slab in curnode.info.items()])\n",
    "            if not curnode.leaf:\n",
    "                s+=\"%d [label=\\\"%s <= %f\\n IG=%f\\n \" %(idx,curnode.split.idvar \\\n",
    "                    if not dic_var else dic_var[curnode.split.idvar],curnode.split.threshold,curnode.split.gain)\n",
    "                s+= \" %s \\n \\\",shape=\\\"box\\\" ];\\n\"  % (labinfo,)\n",
    "                lidx = cpt +1\n",
    "                ridx = cpt +2\n",
    "                s+= \"%d -> %d; %d -> %d;\\n\" % (idx,lidx,idx,ridx)\n",
    "                cpt+=2\n",
    "                nodes += [(curnode.left,lidx),(curnode.right,ridx)]\n",
    "            else:\n",
    "                s+= \"%d [label=\\\"label=%s\\n %s \\\"];\\n\" %(idx,curnode.label,labinfo)\n",
    "        return s+\"}\"\n",
    "\n",
    "    def to_pdf(self,filename,dic_var=None):\n",
    "        pydot.graph_from_dot_data(self.to_dot(dic_var))[0].write_pdf(filename)\n",
    "\n",
    "    def print_tree(self,fields=None):\n",
    "        s=\"\"\n",
    "        nodes=[self.root]\n",
    "        while len(nodes)>0:\n",
    "            curnode=nodes.pop()\n",
    "            if type(curnode)==str:\n",
    "                s+=curnode\n",
    "            else:\n",
    "                if not curnode.leaf:\n",
    "                    s+= \"\\t\"*curnode.depth + \"var %s :  > %f \\n\"  %(str(curnode.split.idvar) if not fields else fields[curnode.split.idvar],curnode.split.threshold)\n",
    "                    nodes+=[curnode.left, \"\\t\"*curnode.depth + \"var %s :  <= %f \\n\"  %(str(curnode.split.idvar) if not fields else fields[curnode.split.idvar],curnode.split.threshold), curnode.right]\n",
    "                else:\n",
    "                    s+= \"\\t\"*curnode.depth + \"class : %s %s\\n\" %(curnode.label,str(curnode.info))\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sur la base de données imdb, apprendre quelques arbres de profondeurs différentes. Visualiser-les. Que remarquez-vous quant au nombre d’exemples séparés à chaque niveau en fonction de la profondeur ? est-ce normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736429038587312\n",
      "var Drama :  > 0.500000 \n",
      "\tvar budget :  > 15660000.000000 \n",
      "\t\tvar Duree :  > 115.500000 \n",
      "\t\t\tvar budget :  > 36500000.000000 \n",
      "\t\t\t\tvar Duree :  > 137.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 101, -1: 20}\n",
      "\t\t\t\tvar Duree :  <= 137.500000 \n",
      "\t\t\t\t\tclass : 1 {-1: 94, 1: 109}\n",
      "\t\t\tvar budget :  <= 36500000.000000 \n",
      "\t\t\t\tvar Sci-Fi :  > 0.500000 \n",
      "\t\t\t\t\tclass : -1 {-1: 5, 1: 4}\n",
      "\t\t\t\tvar Sci-Fi :  <= 0.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 252, -1: 31}\n",
      "\t\tvar Duree :  <= 115.500000 \n",
      "\t\t\tvar Biography :  > 0.500000 \n",
      "\t\t\t\tvar annee :  > 2011.500000 \n",
      "\t\t\t\t\tclass : -1 {-1: 2}\n",
      "\t\t\t\tvar annee :  <= 2011.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 24, -1: 2}\n",
      "\t\t\tvar Biography :  <= 0.500000 \n",
      "\t\t\t\tvar Thriller :  > 0.500000 \n",
      "\t\t\t\t\tclass : -1 {-1: 162, 1: 66}\n",
      "\t\t\t\tvar Thriller :  <= 0.500000 \n",
      "\t\t\t\t\tclass : -1 {1: 135, -1: 149}\n",
      "\tvar budget :  <= 15660000.000000 \n",
      "\t\tvar annee :  > 1973.500000 \n",
      "\t\t\tvar Duree :  > 104.500000 \n",
      "\t\t\t\tvar budget :  > 11158052.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 131, -1: 39}\n",
      "\t\t\t\tvar budget :  <= 11158052.500000 \n",
      "\t\t\t\t\tclass : 1 {-1: 25, 1: 321}\n",
      "\t\t\tvar Duree :  <= 104.500000 \n",
      "\t\t\t\tvar Thriller :  > 0.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 57, -1: 51}\n",
      "\t\t\t\tvar Thriller :  <= 0.500000 \n",
      "\t\t\t\t\tclass : 1 {1: 239, -1: 75}\n",
      "\t\tvar annee :  <= 1973.500000 \n",
      "\t\t\tclass : 1 {1: 175}\n",
      "var Drama :  <= 0.500000 \n",
      "\tvar annee :  > 1984.500000 \n",
      "\t\tvar Duree :  > 110.500000 \n",
      "\t\t\tvar Duree :  > 134.500000 \n",
      "\t\t\t\tvar Duree :  > 177.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 14}\n",
      "\t\t\t\tvar Duree :  <= 177.000000 \n",
      "\t\t\t\t\tclass : 1 {-1: 26, 1: 74}\n",
      "\t\t\tvar Duree :  <= 134.500000 \n",
      "\t\t\t\tvar annee :  > 1992.500000 \n",
      "\t\t\t\t\tclass : -1 {1: 175, -1: 184}\n",
      "\t\t\t\tvar annee :  <= 1992.500000 \n",
      "\t\t\t\t\tclass : 1 {-1: 9, 1: 30}\n",
      "\t\tvar Duree :  <= 110.500000 \n",
      "\t\t\tvar Animation :  > 0.500000 \n",
      "\t\t\t\tvar budget :  > 19000000.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 84, -1: 55}\n",
      "\t\t\t\tvar budget :  <= 19000000.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 12, -1: 1}\n",
      "\t\t\tvar Animation :  <= 0.500000 \n",
      "\t\t\t\tvar budget :  > 6160000.000000 \n",
      "\t\t\t\t\tclass : -1 {-1: 866, 1: 247}\n",
      "\t\t\t\tvar budget :  <= 6160000.000000 \n",
      "\t\t\t\t\tclass : -1 {-1: 123, 1: 99}\n",
      "\tvar annee :  <= 1984.500000 \n",
      "\t\tvar annee :  > 1965.500000 \n",
      "\t\t\tvar Horror :  > 0.500000 \n",
      "\t\t\t\tvar Duree :  > 86.500000 \n",
      "\t\t\t\t\tclass : 1 {-1: 20, 1: 32}\n",
      "\t\t\t\tvar Duree :  <= 86.500000 \n",
      "\t\t\t\t\tclass : -1 {1: 1, -1: 7}\n",
      "\t\t\tvar Horror :  <= 0.500000 \n",
      "\t\t\t\tvar budget :  > 33250000.000000 \n",
      "\t\t\t\t\tclass : -1 {-1: 7, 1: 2}\n",
      "\t\t\t\tvar budget :  <= 33250000.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 122, -1: 29}\n",
      "\t\tvar annee :  <= 1965.500000 \n",
      "\t\t\tvar budget :  > 265500.000000 \n",
      "\t\t\t\tclass : 1 {1: 85}\n",
      "\t\t\tvar budget :  <= 265500.000000 \n",
      "\t\t\t\tvar annee :  > 1950.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 4, -1: 3}\n",
      "\t\t\t\tvar annee :  <= 1950.000000 \n",
      "\t\t\t\t\tclass : 1 {1: 7}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree()\n",
    "dt.max_depth = 5 # on fixe la  taille  de l’arbre a 5\n",
    "dt.min_samples_split = 2 # nombre  minimum d’exemples  pour  spliter  un noeud\n",
    "dt.fit(datax ,datay)\n",
    "dt.predict(datax [:5 ,:])\n",
    "print(dt.score(datax ,datay)) # dessine l’arbre dans un  fichier  pdf si pydot  est  installe.\n",
    "#dt.to_pdf(\"test_tree.pdf\",fields) # sinon  utiliser  http :// www.webgraphviz.com/\n",
    "dt.to_dot(fields) #ou dans la  console\n",
    "print(dt.print_tree(fields ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Calculer les scores de bonnes classification. Comment ils évoluent en fonction de la profondeur ? Est-ce normal ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1923ad0aeb5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Evolution du score en fonction de la profondeur\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tab:blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getScores(datax, labels, nprof):\n",
    "    dt = DecisionTree()\n",
    "    scores = []\n",
    "    for i in range(1, nprof+1):\n",
    "        dt.max_depth = i # on fixe la  taille  de l’arbre a 5\n",
    "        dt.min_samples_split = 2 # nombre  minimum d’exemples  pour  spliter  un noeud\n",
    "        dt.fit(datax, labels)\n",
    "        dt.predict(datax [:5 ,:])\n",
    "        scores.append(dt.score(datax, labels))\n",
    "    return scores\n",
    "\n",
    "# scores = getScores(datax, datay, 10)\n",
    "\n",
    "# Affichage évolution des scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.xlabel(\"profondeur\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"Evolution du score en fonction de la profondeur\")\n",
    "ax.plot(np.arange(len(scores)), scores, color='tab:blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Plus la profondeur est grande et plus le score augmente, c'est tout à fait normal car notre arbre de décision \"se cale sur nos données\" rendant la prédiction de plus en plus précise => sur-apprentissage </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Ces scores sont-ils un indicateur fiable du comportement de l’algorithme ? Comment obtenir un indicateur plus fiable ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Ces scores ne sont évidemment pas fiables car nos données qui nous servent à faire la prédiction sont les mêmes qui nous servent d'apprentissage, il faudrait séparer nos données en deux parties : apprentissage et test </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><code style=\"background:yellow; color:black\"> Sur et sous apprentissage </code></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir une meilleure estimation de l’erreur du classifieur appris, il est usuel d’utiliser deux ensembles d’exemples étiquetés :\n",
    "<ul>\n",
    "    <li>l’ensemble d’apprentissage : l’apprentissage du classifieur ne se fait que sur ce sous-ensemble d’exemples ; </li>\n",
    "    <li>l’ensemble de test : cet ensemble sert à évaluer l’erreur du classifieur. </li> \n",
    "</ul>\n",
    "Ces deux sous-ensembles sont tirés de manière aléatoire en faisant une partition en 2 parties des exemples disponibles. L’erreur faîte sur l’ensemble d’apprentissage s’appelle l’erreur d’apprentissage, celle sur l’ensemble de test l’erreur de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Pour différents partitionnement, par exemple des partages en (0.2,0.8), (0.5,0.5), (0.8,0.2), tracer les courbes de l’erreur en apprentissage et de l’erreur en test en fonction de la profondeur du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainTest(data, labels, pc_train):\n",
    "    index = np.random.permutation(len(labels))\n",
    "    napp = int(len(labels)*pc_train) \n",
    "    X_train, y_train = data[index[:napp]], labels[index[:napp]]\n",
    "    X_test, y_test   = data[index[napp:]], labels[index[napp:]]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getErrors(data, labels, nprof, pc_train):\n",
    "    dt = DecisionTree()\n",
    "    errApp = []\n",
    "    errTest = []\n",
    "    X_train, y_train, X_test, y_test = splitTrainTest(data, labels, pc_train)\n",
    "    for i in range(1, nprof+1):\n",
    "        dt.max_depth = i # on fixe la  taille  de l’arbre a 5\n",
    "        dt.min_samples_split = 2 # nombre  minimum d’exemples  pour  spliter  un noeud\n",
    "        dt.fit(X_train, y_train)\n",
    "        dt.predict(X_test)\n",
    "        errApp.append(1 - dt.score(X_train, y_train))\n",
    "        errTest.append(1 - dt.score(X_test, y_test))\n",
    "    return errApp, errTest\n",
    "\n",
    "errApp_08, errTest_02 = getErrors(datax, datay, 20, 0.8)\n",
    "errApp_05, errTest_05 = getErrors(datax, datay, 20, 0.5)\n",
    "errApp_02, errTest_08 = getErrors(datax, datay, 20, 0.2)\n",
    "\n",
    "# Affichage évolution des erreurs\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.xlabel(\"profondeur\")\n",
    "plt.ylabel(\"erreur\")\n",
    "plt.title(\"Evolution de l'erreur en fonction de la profondeur 0.2/0.8\")\n",
    "ax.plot(np.arange(len(errApp_02)), errApp_02, color='tab:blue')\n",
    "ax.plot(np.arange(len(errTest_08)), errTest_08, color='tab:orange')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.xlabel(\"profondeur\")\n",
    "plt.ylabel(\"erreur\")\n",
    "plt.title(\"Evolution de l'erreur en fonction de la profondeur 0.5/0.5\")\n",
    "ax.plot(np.arange(len(errApp_05)), errApp_05, color='tab:blue')\n",
    "ax.plot(np.arange(len(errTest_05)), errTest_05, color='tab:orange')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.xlabel(\"profondeur\")\n",
    "plt.ylabel(\"erreur\")\n",
    "plt.title(\"Evolution de l'erreur en fonction de la profondeur 0.8/0.2\")\n",
    "ax.plot(np.arange(len(errApp_08)), errApp_08, color='tab:blue')\n",
    "ax.plot(np.arange(len(errTest_02)), errTest_02, color='tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Que remarquez vous quand il y a peu d’exemples d’apprentissage ? Comment progresse l’erreur ? De même quand il y a beaucoup d’exemples d’apprentissage. Est-ce le même comportement pour les deux erreurs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Lorsqu'il y a peu d'exemples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Vos résultats vous semblent ils fiables et stables ? Comment les améliorer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Non. Cross-validation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><code style=\"background:yellow; color:black\"> Validation croisée : sélection de modèle </code></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est rare de disposer en pratique d’un ensemble de test (on préfère inclure le plus grand nombre de données dans l’ensemble d’apprentissage). Pour sélectionner un modèle tout en considérant le plus grand nombre d’exemples possible pour l’apprentissage, on utilise généralement une procédure dite de sélection par validation croisée. Pour chaque paramétrisation de l’algorithme, une estimation de l’erreur empirique du classifieur appris est faîte selon la procédure suivante :\n",
    "<ul>\n",
    "    <li>l’ensemble d’apprentissage $E_{app}$ est partitionné en *n* ensembles d’apprentissage ${E_i}$ </li>\n",
    "    <li>Pour $i = 1..n$ </li>\n",
    "        <ul>\n",
    "            <li> l’arbre est appris sur $E_{app}$ \\ $E_i$ </li>\n",
    "            <li> l’erreur en test $err(E_i)$ est évaluée sur $E_i$ (qui n’a pas servi à l’apprentissage à cette itération) </li>\n",
    "            <li> l’erreur moyenne $err = \\frac{1}{n} \\sum_{i=1}^{n}err(E_i)$ est calculée, le modèle sélectionné est celui qui minimise cette erreur </li>\n",
    "        </ul>\n",
    "</ul>\n",
    "<br>\n",
    "Refaire les expériences précédentes avec cette fois de la validation croisée (penser à la fonction **np.random.shuffle**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(data, labels):\n",
    "    index = np.random.shuffle(np.arange(len(labels)))\n",
    "    data_shu = data[index]\n",
    "    labels_shu = labels[index]\n",
    "    return data_shu, labels_shu\n",
    "\n",
    "def cross_validation(data, labels, n, nprof):\n",
    "    # Step 1 : Shuffle\n",
    "    data_shu, labels_shu = shuffle(data, labels)\n",
    "    \n",
    "    # Step 2 : split equals parts\n",
    "    inter = np.linspace(0, len(labels), n+1)\n",
    "    print(inter)\n",
    "    \n",
    "    # Step 3 : initiate decision tree\n",
    "    dt = DecisionTree()\n",
    "    dt.max_depth = nprof\n",
    "    dt.min_samples_split = 2 #nombre  minimum d’exemples  pour  spliter  un noeud\n",
    "    mean_err = []\n",
    "    \n",
    "    # Step 4 : iterate over data\n",
    "    for i in range(n):\n",
    "        test_x, test_y = data_shu[0][ int(inter[i]): int(inter[i+1]) ], labels_shu[0][ int(inter[i]) : int(inter[i+1]) ]\n",
    "        if i == 0:\n",
    "            train_x, train_y = data_shu[0][ int(inter[1]) : int(inter[n-1]) ], labels_shu[0][ int(inter[1]) : int(inter[n-1]) ]\n",
    "        elif i == n-2:\n",
    "            train_x, train_y = data_shu[0][ int(inter[0]) : int(inter[n-2]) ], labels_shu[0][ int(inter[0]) : int(inter[n-2])]\n",
    "        else:\n",
    "            train_x = np.vstack( ( data_shu[0][ int(inter[0]) : int(inter[i]) ], data_shu[0][ int(inter[i+1]) : int(inter[n-1]) ] ) ) \n",
    "            train_y = np.hstack( ( labels_shu[0][ int(inter[0]) : int(inter[i]) ], labels_shu[0][ int(inter[i+1]) : int(inter[n-1]) ] ) )\n",
    "            \n",
    "        dt.fit(train_x, train_y)\n",
    "        dt.predict(test_x)\n",
    "        mean_err.append(1 - dt.score(test_x, test_y))\n",
    "        \n",
    "    return np.array(mean_err).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_err = []\n",
    "for i in range(1, 11):\n",
    "    mean_err.append(cross_validation(datax, datay, 10, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x285d6521400>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.xlabel(\"profondeur\")\n",
    "plt.ylabel(\"erreur moy\")\n",
    "plt.title(\"Evolution de l'erreur moy selon la profondeur ; CV kfold = 10\")\n",
    "ax.plot(np.arange(1,len(mean_err)+1), mean_err, color='tab:blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
